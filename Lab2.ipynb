{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally developed by instructor Sophia (Yakun) Shao and TAs Alon Amid & Hasan Genc from UC Berkeley, 2020 Spring, \"Hardware for Machine Learning\". Adpated by 2024 Fall \"AI Computing Systems\" teaching team.\n",
    "\n",
    "You are required to fill in the blanks with your code **independently** to complete the quantization process (weights, activations and biases quantization), so we can simulate and verify the quantized NN model on floating-point hardware (CPUs or GPGPUs). It provides us the confidence that the model can be run on a customized 8-bit hardware with limited loss of accuracy. \n",
    "\n",
    "**Throughout this experiment, we use symmetric quantization, which means the \"zero point\" should be 0. Please make sure your results are reproducible and do not change the structure of the CNN (1 point)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning the assignment, we import the CIFAR dataset, and train the simple convolutional neural network (CNN) built in Lab1 to classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** set the runtime type to \"GPU\" if it is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe94da77930>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "torch.manual_seed(2024219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and test data from the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/home/ubuntu/data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=False, num_workers=8)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/home/ubuntu/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple CNN that classifies CIFAR images is built in Lab 1 and has the has the following architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Layer** | **Type** | **Input Shape** | **Output Shape** | **Activation**\n",
    "--- | --- | --- | --- | ---\n",
    "conv1 | Convolutional | 3x32x32 | 12x28x28 | ReLU \n",
    "pool1 | Max pool | 12x28x28 | 12x14x14 | None                \n",
    "conv2 | Convolutional | 12x14x14 | 32x12x12 | ReLU                \n",
    "pool2 | Max pool | 32x12x12 | 32x6x6 | None                \n",
    "fc1 | Fully-connected | 1152 | 256 | ReLU                \n",
    "fc2 | Fully-connected | 256 | 64 | ReLU                \n",
    "fc3 | Fully-connected | 64 | 10 | None                \n",
    "\n",
    "None of the layers in the network have a bias associated with them.\n",
    "This makes them easier to quantize.\n",
    "Towards the end of this assignment, we will add biases to the final layer and quantize it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2) # run after each conv (hence the 5x5 FC layer)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, bias=False)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 256, bias=False)\n",
    "        self.fc2 = nn.Linear(256, 64, bias=False)\n",
    "        self.fc3 = nn.Linear(64, 10, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training and testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: nn.Module, dataloader: DataLoader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.015, momentum=0.9)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple (2) times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:    # print every 100 mini-batches\n",
    "                print('[Epoch: %d, Iteration: %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    n_inferences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images) # get 1 batch worth of image predictions (i.e. 4 predictions of 10 each)\n",
    "            other, predicted = torch.max(outputs.data, 1) # other == values, predicted == indicies\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if max_samples:\n",
    "                n_inferences += images.shape[0]\n",
    "                if n_inferences > max_samples:\n",
    "                    break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train this CNN on the training dataset (this may take 20 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Iteration:   100] loss: 2.278\n",
      "[Epoch: 1, Iteration:   200] loss: 1.994\n",
      "[Epoch: 1, Iteration:   300] loss: 1.716\n",
      "[Epoch: 1, Iteration:   400] loss: 1.614\n",
      "[Epoch: 1, Iteration:   500] loss: 1.522\n",
      "[Epoch: 2, Iteration:   100] loss: 1.431\n",
      "[Epoch: 2, Iteration:   200] loss: 1.394\n",
      "[Epoch: 2, Iteration:   300] loss: 1.301\n",
      "[Epoch: 2, Iteration:   400] loss: 1.267\n",
      "[Epoch: 2, Iteration:   500] loss: 1.226\n",
      "[Epoch: 3, Iteration:   100] loss: 1.160\n",
      "[Epoch: 3, Iteration:   200] loss: 1.157\n",
      "[Epoch: 3, Iteration:   300] loss: 1.082\n",
      "[Epoch: 3, Iteration:   400] loss: 1.066\n",
      "[Epoch: 3, Iteration:   500] loss: 1.029\n",
      "[Epoch: 4, Iteration:   100] loss: 0.990\n",
      "[Epoch: 4, Iteration:   200] loss: 0.988\n",
      "[Epoch: 4, Iteration:   300] loss: 0.931\n",
      "[Epoch: 4, Iteration:   400] loss: 0.931\n",
      "[Epoch: 4, Iteration:   500] loss: 0.893\n",
      "[Epoch: 5, Iteration:   100] loss: 0.875\n",
      "[Epoch: 5, Iteration:   200] loss: 0.858\n",
      "[Epoch: 5, Iteration:   300] loss: 0.807\n",
      "[Epoch: 5, Iteration:   400] loss: 0.818\n",
      "[Epoch: 5, Iteration:   500] loss: 0.778\n",
      "[Epoch: 6, Iteration:   100] loss: 0.768\n",
      "[Epoch: 6, Iteration:   200] loss: 0.760\n",
      "[Epoch: 6, Iteration:   300] loss: 0.733\n",
      "[Epoch: 6, Iteration:   400] loss: 0.736\n",
      "[Epoch: 6, Iteration:   500] loss: 0.701\n",
      "[Epoch: 7, Iteration:   100] loss: 0.667\n",
      "[Epoch: 7, Iteration:   200] loss: 0.669\n",
      "[Epoch: 7, Iteration:   300] loss: 0.664\n",
      "[Epoch: 7, Iteration:   400] loss: 0.684\n",
      "[Epoch: 7, Iteration:   500] loss: 0.660\n",
      "[Epoch: 8, Iteration:   100] loss: 0.597\n",
      "[Epoch: 8, Iteration:   200] loss: 0.615\n",
      "[Epoch: 8, Iteration:   300] loss: 0.630\n",
      "[Epoch: 8, Iteration:   400] loss: 0.630\n",
      "[Epoch: 8, Iteration:   500] loss: 0.589\n",
      "[Epoch: 9, Iteration:   100] loss: 0.554\n",
      "[Epoch: 9, Iteration:   200] loss: 0.575\n",
      "[Epoch: 9, Iteration:   300] loss: 0.589\n",
      "[Epoch: 9, Iteration:   400] loss: 0.537\n",
      "[Epoch: 9, Iteration:   500] loss: 0.538\n",
      "[Epoch: 10, Iteration:   100] loss: 0.501\n",
      "[Epoch: 10, Iteration:   200] loss: 0.546\n",
      "[Epoch: 10, Iteration:   300] loss: 0.514\n",
      "[Epoch: 10, Iteration:   400] loss: 0.477\n",
      "[Epoch: 10, Iteration:   500] loss: 0.508\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the CNN has been trained, let's test it on our test dataset. The test accuracy should be above 60% after 10 epochs of training.  **Report this accuracy (1 point).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 64.06%\n"
     ]
    }
   ],
   "source": [
    "score = test(net, testloader)\n",
    "print('Accuracy of the network on the test images: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convenience function which we use to copy CNN's for version control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def copy_model(model: nn.Module) -> nn.Module:\n",
    "    result = deepcopy(model)\n",
    "\n",
    "    # Copy over the extra metadata we've collected which copy.deepcopy doesn't capture\n",
    "    if hasattr(model, 'input_activations'):\n",
    "        result.input_activations = deepcopy(model.input_activations)\n",
    "\n",
    "    for result_layer, original_layer in zip(result.children(), model.children()):\n",
    "        if isinstance(result_layer, nn.Conv2d) or isinstance(result_layer, nn.Linear):\n",
    "            if hasattr(original_layer.weight, 'scale'):\n",
    "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
    "            if hasattr(original_layer, 'activations'):\n",
    "                result_layer.activations = deepcopy(original_layer.activations)\n",
    "            if hasattr(original_layer, 'output_scale'):\n",
    "                result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Visualize Weights\n",
    "### Question 1.1:\n",
    "\n",
    "Plot histograms of the weights of every convolutional and fully-connected layer. Record any observations you make about the distribution of the values and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible required lib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADD YOUR CODE HERE to plot distributions of weights of the original NN model. Add them to the report (0.5 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ADD YOUR CODE HERE\n",
    "\n",
    "# You can get a flattened vector of the weights of fc1 like this:\n",
    "# Try plotting a histogram of fc1_weights (and the weights of all the other layers as well)\n",
    "conv1_weights = net.conv1.weight.data.cpu().view(-1)\n",
    "conv2_weights = net.conv2.weight.data.cpu().view(-1)\n",
    "fc1_weights = net.fc1.weight.data.cpu().view(-1)\n",
    "fc2_weights = net.fc2.weight.data.cpu().view(-1)\n",
    "fc3_weights = net.fc3.weight.data.cpu().view(-1)\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "\n",
    "axs[0, 0].hist(conv1_weights, bins=256)\n",
    "axs[0, 0].set_title(\"conv1\")\n",
    "\n",
    "axs[0, 1].hist(conv2_weights, bins=256)\n",
    "axs[0, 1].set_title(\"conv2\")\n",
    "\n",
    "axs[0, 2].axis('off')\n",
    "\n",
    "axs[1, 0].hist(fc1_weights, bins=256)\n",
    "axs[1, 0].set_title(\"fc1\")\n",
    "\n",
    "axs[1, 1].hist(fc2_weights, bins=256)\n",
    "axs[1, 1].set_title(\"fc2\")\n",
    "\n",
    "axs[1, 2].hist(fc3_weights, bins=256)\n",
    "axs[1, 2].set_title(\"fc3\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# You can use \"hist\" from the matplotlib.pyplot package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2:\n",
    "\n",
    "Record the range of the weights, as well as their 3-sigma range (the difference between $\\mu + 3\\sigma$ and $\\mu - 3\\sigma$).\n",
    "For which layers is the 3-sigma range larger or smaller than the actual range?\n",
    "\n",
    "**ADD YOUR CODE HERE to record the 3-sigma and actual range of the weights in each layer. Report the results (0.5 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful functions: torch.min/max/mean/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Quantize Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any convolution or fully-connected layer pass, without a bias, can be described by the equation:\n",
    "\n",
    "$$W*In = Out$$\n",
    "\n",
    "where $W$ is the weight tensor, $In$ in the input tensor, and $Out$ is the output tensor.\n",
    "\n",
    "For this question, your task is to find a *scaling factor*, called $s_W$ for each convolutional and fully connected layer,\n",
    "which would fit inside an 8-bit signed integer.\n",
    "This equation can now be described as the following:\n",
    "\n",
    "<!-- $$/*In = Out/s_W$$ -->\n",
    "$$\\frac{W}{s_W}*In = \\frac{Out}{s_W}$$\n",
    "\n",
    "You might wonder: \"Isn't it a problem that the output of the layer has now changed? Wouldn't quantizing the weights change the output of the neural net?\"\n",
    "\n",
    "The answer, of course, is: \"Yes\".\n",
    "However, what we care about is not the *absolute* values output by the CNN, but the relative difference between the probabilities it assigns to different classes for its predictions.\n",
    "Quantizing the weights only scales this relative difference up or down, but it does not affect which class the network assigns the most probability to.\n",
    "Therefore, it does not affect the final predictions that the neural net makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the original model into a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_q2 = copy_model(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1:\n",
    "\n",
    "**Fill in the `quantized_weights` function.**\n",
    "The template code we provide will then call this function on the weights of every layer in the CNN that we just trained at 32-bit floating point precision, to lower them into 8-bit signed integer (2'complement, -128~127) precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def quantized_weights(weights: torch.Tensor) -> Tuple[torch.Tensor, float]:\n",
    "    '''\n",
    "    Quantize the weights so that all values are integers between -128 and 127.\n",
    "    You may want to use the maxmimum absolute value of total range, 3-sigma range, \n",
    "    or other ranges for symmetric quantization when deciding just what factors to \n",
    "    scale the float32 values by.\n",
    "\n",
    "    Parameters:\n",
    "    weights (Tensor): The unquantized weights\n",
    "\n",
    "    Returns:\n",
    "    (Tensor, float): A tuple with the following elements:\n",
    "                        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
    "                          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
    "                        * The scaling factor that your weights were multiplied by.\n",
    "                          This value does not need to be an 8-bit integer.\n",
    "    '''\n",
    "\n",
    "    ''' \n",
    "    ADD YOUR CODE HERE to compute 'result' and change 'scale'. \n",
    "    We consider only symmetric quantization, which means zeros point is exactly 0.\n",
    "    After that, return the 'scale' together with the quantized number, 'result', in [-128,127].\n",
    "    '''\n",
    "    scale = 1\n",
    "    result = 0\n",
    "    return torch.clamp(result, min=-128, max=127), scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_layer_weights(model: nn.Module):\n",
    "    # Quantize the weights layer by layer and record the scale factors and quantized weights\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            # Quantize the weights using the function you just developed\n",
    "            q_layer_data, scale = quantized_weights(layer.weight.data)\n",
    "            q_layer_data = q_layer_data.to(device)\n",
    "\n",
    "            layer.weight.data = q_layer_data\n",
    "            layer.weight.scale = scale\n",
    "            \n",
    "            # Check if the weights are quantized properly, your code should be okay if no exception is raised.\n",
    "            # Otherwise, please check your code.\n",
    "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
    "            if (q_layer_data != q_layer_data.round()).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
    "\n",
    "quantize_layer_weights(net_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2:\n",
    "\n",
    "**Record the accuracy change of the network after quantizing its weights and report. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = test(net_q2, testloader)\n",
    "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Visualize Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have quantized the weights of the CNN, we must also quantize the activations (inputs and outputs to layers) traveling through it.\n",
    "But before doing so, let's analyze what values the activations take when travelling through the network.\n",
    "\n",
    "We provide convenience code which will record the values of every pixel of the outputs and inputs travelling through the neural network.\n",
    "(This is the initial CNN, where not even the weights had yet been quantized).\n",
    "We then profile these values when running on a subset of the training set (calibration data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_activation_profiling_hooks(model: Net):\n",
    "    # initialize the activations with empty ndarray\n",
    "    model.input_activations = np.empty(0)\n",
    "    model.conv1.activations = np.empty(0)\n",
    "    model.conv2.activations = np.empty(0)\n",
    "    model.fc1.activations = np.empty(0)\n",
    "    model.fc2.activations = np.empty(0)\n",
    "    model.fc3.activations = np.empty(0)\n",
    "\n",
    "    model.profile_activations = True\n",
    "\n",
    "    def conv1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.input_activations = np.append(model.input_activations, x[0].cpu().view(-1))\n",
    "    model.conv1.register_forward_hook(conv1_activations_hook)\n",
    "    # function register_forward_hook is used to obtain the activations, which would otherwise be abandoned as\n",
    "    # intermediate variables. For more details, please refer to \n",
    "    # https://pytorch.org/docs/stable/notes/modules.html#module-initialization\n",
    "    # Module Hooks\n",
    "\n",
    "    def conv2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv1.activations = np.append(model.conv1.activations, x[0].cpu().view(-1))\n",
    "    model.conv2.register_forward_hook(conv2_activations_hook)\n",
    "\n",
    "    def fc1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv2.activations = np.append(model.conv2.activations, x[0].cpu().view(-1))\n",
    "    model.fc1.register_forward_hook(fc1_activations_hook)\n",
    "\n",
    "    def fc2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc1.activations = np.append(model.fc1.activations, x[0].cpu().view(-1))\n",
    "    model.fc2.register_forward_hook(fc2_activations_hook)\n",
    "\n",
    "    def fc3_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc2.activations = np.append(model.fc2.activations, x[0].cpu().view(-1))\n",
    "            model.fc3.activations = np.append(model.fc3.activations, y[0].cpu().view(-1))\n",
    "    model.fc3.register_forward_hook(fc3_activations_hook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the unquantized model to profile input and output activations on a subset of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_q3 = copy_model(net)\n",
    "# activate the hook record activations\n",
    "register_activation_profiling_hooks(net_q3)\n",
    "\n",
    "# Run through the training dataset again while profiling the input and output activations this time\n",
    "# We don't actually have to perform gradient descent for this, so we can use the \"test\" function\n",
    "test(net_q3, trainloader, max_samples=1000)\n",
    "net_q3.profile_activations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the activations\n",
    "input_activations = net_q3.input_activations\n",
    "conv1_output_activations = net_q3.conv1.activations\n",
    "conv2_output_activations = net_q3.conv2.activations\n",
    "fc1_output_activations = net_q3.fc1.activations\n",
    "fc2_output_activations = net_q3.fc2.activations\n",
    "fc3_output_activations = net_q3.fc3.activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1:\n",
    "\n",
    "**Plot histograms of the input images and the outputs of every convolutional and fully-connected layer. \n",
    "Record any observations you make about the distribution of the values. (0.5 point)** Remember that you are plotting the activations *after* activation functions like ReLU have been applied, which means that you should not be worried if you find that your plots are asymmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE to plot distributions of activations\n",
    "# Plot histograms of the following variables:\n",
    "#   input_activations\n",
    "#   conv1_output_activations\n",
    "#   conv2_output_activations\n",
    "#   fc1_output_activations\n",
    "#   fc2_output_activations\n",
    "#   fc3_output_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2:\n",
    "\n",
    "Additionally, **report the range of the values, as well as their 3-sigma range (the difference between $\\mu + 3\\sigma$ and $\\mu - 3\\sigma$) (0.5 point)**.\n",
    "For which layers is the 3-sigma range larger or smaller than the actual range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE to record the range and 3-sigma range of the activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Quantize Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to quantize the activations (inputs and outputs to layers) traveling through the CNN.\n",
    "Our equation now becomes:\n",
    "\n",
    "$$\\frac{W}{s_W}*\\frac{In}{s_{In}}*\\frac{1}{s_{Out} }= \\frac{Out}{s_Ws_{In}s_{Out}}$$\n",
    "\n",
    "where $s_{In}$ is the scaling factor which was applied to the input to the layer, and $s_{Out}$ is the scaling factor which we decide to apply to the output of the layer.\n",
    "$s_{Out}$ must be chosen such that the expected values of the elements of $Out$ can be scaled down to fit within 8 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1:\n",
    "\n",
    "Before performing any quantization at all, we could describe the output of the `conv1` layer as:\n",
    "\n",
    "$$W_{conv1} * In = Out_{conv1}$$\n",
    "\n",
    "Suppose that we quantized the input matrix, $In$, scaling it by $s_{In}$.\n",
    "Suppose that we also scaled the weight matrix, $W_{conv1}$, by $s_{W_{conv1}}$, and the output matrix, $Out_{conv1}$, by $s_{Out_{conv1}}$.\n",
    "\n",
    "**In the lab report answer the following sub-questions for 4.1 (1 point).**:\n",
    "\n",
    "**(a)** Write an equation describing the output of the `conv1` layer with these new scaling parameters.\n",
    "\n",
    "**(b)** Write an equation describing the output of the `conv2` layer in terms of $In$, $W_{conv1}$, $W_{conv2}$, $Out_{conv1}$, $Out_{conv2}$, $s_{In}$, $s_{W_{conv1}}$, $s_{W_{conv2}}$, $s_{Out_{conv1}}$, and $s_{Out_{conv2}}$.\n",
    "You can pretend that the pooling layers do not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2:\n",
    "\n",
    "**Complete the `quantize_initial_input` and `quantize_activations` functions which calculate the scaling factors for the initial image which is input to the CNN, and the outputs of each layer, respectively.** Again you can use 3-sigma range or min-max range.\n",
    "\n",
    "### Question 4.3:\n",
    "\n",
    "**Complete the `forward` function for the `NetQuantized` class.\n",
    "You will have to add code here to scale the outputs of each layer, and then to clamp the outputs of each layer to integers between -128 and 127 afterwards.**\n",
    "\n",
    "**Be careful with the data type (ndarray/tensor/scalar/tuple/float) specified and required by each function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class NetQuantized(nn.Module):\n",
    "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
    "        super(NetQuantized, self).__init__()\n",
    "        \n",
    "        net_init = copy_model(net_with_weights_quantized)\n",
    "\n",
    "        self.conv1 = net_init.conv1\n",
    "        self.pool = net_init.pool\n",
    "        self.conv2 = net_init.conv2\n",
    "        self.fc1 = net_init.fc1\n",
    "        self.fc2 = net_init.fc2\n",
    "        self.fc3 = net_init.fc3\n",
    "\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            def pre_hook(l, x):\n",
    "                x = x[0]\n",
    "                if (x < -128).any() or (x > 127).any():\n",
    "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
    "                if (x != x.round()).any():\n",
    "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
    "\n",
    "            layer.register_forward_pre_hook(pre_hook)\n",
    "\n",
    "        # Calculate the scaling factor for the initial input to the CNN\n",
    "        self.input_activations = net_with_weights_quantized.input_activations\n",
    "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
    "\n",
    "        # Calculate the output scaling factors for all the layers of the CNN\n",
    "        preceding_layer_scales = []\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            layer.output_scale = NetQuantized.quantize_activations(layer.activations, layer.weight.scale, self.input_scale, preceding_layer_scales)\n",
    "            preceding_layer_scales.append((layer.weight.scale, layer.output_scale))\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_initial_input(pixels: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
    "        Remember to use symmetric scaling (zero-point = 0)\n",
    "        Parameters:\n",
    "        pixels (ndarray): The values of all the pixels which were part of the input image during training\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_activations(activations: np.ndarray, s_w: float, s_initial_input: float, ss: List[Tuple[float, float]]) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor to multiply the output of a layer by.\n",
    "        Remember to use symmetric scaling (zero-point = 0)\n",
    "        Parameters:\n",
    "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
    "        s_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
    "        s_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
    "        ss ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the layer output should be multiplied by before being fed into the first layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "        \n",
    "        #ADD YOUR CODE HERE \n",
    "        return 1.0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # You can access the output activation scales like this:\n",
    "        #   fc1_output_scale = self.fc1.output_scale\n",
    "\n",
    "        # To make sure that the outputs of each layer are integers between -128 and 127, you may need to use the following functions:\n",
    "        #   * torch.round\n",
    "        #   * torch.clamp\n",
    "\n",
    "        # ADD YOUR CODE HERE \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the information from net_q2 and net_q3 together\n",
    "net_init = copy_model(net_q2)  \n",
    "net_init.input_activations = deepcopy(net_q3.input_activations) \n",
    "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
    "    if isinstance(layer_init, nn.Conv2d) or isinstance(layer_init, nn.Linear):\n",
    "        layer_init.activations = deepcopy(layer_q3.activations)\n",
    "\n",
    "net_quantized = NetQuantized(net_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4:\n",
    "\n",
    "Finally, record the accuracy of your network after both weights and activations have been quantized.\n",
    "If you've done everything right, you should still find little accuracy change. **Report the accuracy. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = test(net_quantized, testloader)\n",
    "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Quantize Biases\n",
    "Let us now update our CNN to include a bias in its final layer, *fc3*.\n",
    "We have already included code to create and train a new CNN called `net_with_bias`.\n",
    "\n",
    "Consider how a bias affects the equation for an unquantized layer:\n",
    "\n",
    "$$W * In + \\beta = Out$$\n",
    "\n",
    "where $\\beta$ is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1:\n",
    "\n",
    "Suppose that we again quantized a biased layer with the same scaling factors we used in previous questions: $s_W$, $s_{In}$, and $s_{Out}$.\n",
    "What would we scale $\\beta$ by in this case?\n",
    "**Write an equation in your lab report to describe the output of the quantized layer with a bias (1 point).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new network with a bias on *fc3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWithBias(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithBias, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 12, 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(12, 32, 3, bias=False)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 256, bias=False)\n",
    "        self.fc2 = nn.Linear(256, 64, bias=False)\n",
    "        self.fc3 = nn.Linear(64, 10, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net_with_bias = NetWithBias().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and score the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net_with_bias, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = test(net_with_bias, testloader)\n",
    "print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the network with quantized weights but unquantized bias and **report the accuracy (1 point)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_activation_profiling_hooks(net_with_bias)\n",
    "test(net_with_bias, trainloader, max_samples=400)\n",
    "net_with_bias.profile_activations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_with_bias_with_quantized_weights = copy_model(net_with_bias)\n",
    "quantize_layer_weights(net_with_bias_with_quantized_weights)\n",
    "\n",
    "score = test(net_with_bias_with_quantized_weights, testloader)\n",
    "print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2:\n",
    "\n",
    "Fill in the `quantized_bias` function in the `NetQuantizedWithBias` class.\n",
    "This function is meant to quantize the bias on the final layer of the CNN. You are suggested to use the method covered in the lecture.\n",
    "Keep in mind that biases are typically quantized to 32-bits, so your bias values do not all have to be between -128 and 127 (though 32-bits is a bit conservative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly clearer bias bounds (32b signed integer)\n",
    "MIN_32B_SINT = -(2**31)\n",
    "MAX_32B_SINT = (2**31) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetQuantizedWithBias(NetQuantized): # get quantized weights & activations\n",
    "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
    "        super(NetQuantizedWithBias, self).__init__(net_with_weights_quantized)\n",
    "        \n",
    "        preceding_scales = [(layer.weight.scale, layer.output_scale) for layer in self.children() if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)][:-1]\n",
    "\n",
    "        #print(self.fc3.bias.data)\n",
    "        self.fc3.bias.data = NetQuantizedWithBias.quantized_bias(\n",
    "            self.fc3.bias.data,\n",
    "            self.fc3.weight.scale,\n",
    "            self.input_scale,\n",
    "            preceding_scales\n",
    "        )\n",
    "        #print(self.fc3.bias.data)\n",
    "\n",
    "        if (self.fc3.bias.data < MIN_32B_SINT).any() or (self.fc3.bias.data > MAX_32B_SINT).any():\n",
    "               raise Exception(\"Bias has values which are out of bounds for an 32-bit signed integer\")\n",
    "        if (self.fc3.bias.data != self.fc3.bias.data.round()).any():\n",
    "             raise Exception(\"Bias has non-integer values\")\n",
    "\n",
    "    @staticmethod\n",
    "    def quantized_bias(bias: torch.Tensor, s_w: float, s_initial_input: float, ss: List[Tuple[float, float]]) -> torch.Tensor:\n",
    "        '''\n",
    "        Quantize the bias so that all values are integers between MIN_32B_SINT and MAX_32B_SINT.\n",
    "\n",
    "        Parameters:\n",
    "        bias (Tensor): The floating point values of the bias\n",
    "        s_w (float): The scale by which the weights of this layer were multiplied\n",
    "        s_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
    "        ss ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
    "\n",
    "        Returns:\n",
    "        Tensor: The bias in quantized form, where every value is an integer between MIN_32B_SINT and MAX_32B_SINT.\n",
    "                The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
    "        '''\n",
    "\n",
    "        # ADD YOUR CODE HERE to compute and replace the scale for bias (scale_bias)\n",
    "        \n",
    "        scale_bias = 1\n",
    "\n",
    "        return torch.clamp((bias * scale_bias).round(), min=MIN_32B_SINT, max=MAX_32B_SINT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.3:\n",
    "\n",
    "What is your accuracy before and after quantizing CNN with the bias?\n",
    "The accuracy change should ideally be negligible or at least partially improved. **Report the value. (1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = test(net_quantized_with_bias, testloader)\n",
    "print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bonus:\n",
    "When quantizing weights and activations, please evaluate the impact on model accuracy by comparing the use of the maximum absolute value of the actual range with the maximum absolute value of the 3-sigma range ($\\mu + 3\\sigma$ and $\\mu - 3\\sigma$) for calculating the quantization scale. Include these findings in your report and provide an analysis of the underlying reasons for any observed differences. \n",
    "\n",
    "Additionally, consider employing a third method, such as the entropy-based quantization technique discussed in class. It is recommended that you create a copy of the current script, adjust the scale calculation method in the copy, and re-run the experiments to ensure that your changes do not affect the original results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
